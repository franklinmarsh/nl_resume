{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # NLTK Resume Tool\n",
    " \n",
    " 1. Scrape job description with BeautifulSoup\n",
    " 2. Process text with nltk, return a list of parts of speech and frequency\n",
    " 3. Select the best match from a list of possible resume statements.\n",
    " 4. Construct resume in LaTeX or microsoft word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the python scientific suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Franklin/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.signal\n",
    "\n",
    "from gatspy.periodic import LombScargleFast, LombScargleMultibandFast, LombScargle\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "# This is importing a \"future\" python version 3 print function.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#plotting options\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('paper', font_scale = 1.5)\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Import nltk and re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk #natural language toolkit\n",
    "import re #regular expressions\n",
    "from bs4 import BeautifulSoup #for web scraping\n",
    "import requests #requests for pulling html data\n",
    "\n",
    "#nltk.download() #uncomment this to download the required nltk resources that are not in the anaconda package.\n",
    "#Or in the shell: python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the HTML of a job description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = requests.get('https://jobs.te.com/job/-/-/1122/2719233?apstr=%3Fmode%3Djob%26iis%3DIndeed%26iisn%3DIndeed.com&ss=paid').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the html, we'll create a soup object, and then get a list of strings ('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "data = soup.findAll(text = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some code that I've borrowed from quora, filter the data fesult, to remove style, scripts, heads, etc. Then filter by length to find those job requirements bullets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    " \n",
    "result = filter(visible, data)\n",
    "\n",
    "long_results = [x for x in result if len(x) > 40 and len(x) < 320]\n",
    "len_array = [len(x) for x in long_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noturl = re.compile(r'^(.(?!www))*$')\n",
    "notee =  re.compile(r'^(.(?!equal opportunity))*$') #exclude the equal opportunity statement\n",
    "not_thirdparty =  re.compile(r'^(.(?!placement agencies))*$') #exclude the equal opportunity statement\n",
    "not_cr =  re.compile(r'^(.(?!All rights reserved))*$') #exclude the equal opportunity statement\n",
    "\n",
    "\n",
    "long_results_filtered = [i for i in long_results if noturl.search(i)\\\n",
    "                         and notee.search(i)\\\n",
    "                         and not_thirdparty.search(i)\\\n",
    "                        and not_cr.search(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "single_string = ' '.join(long_results_filtered) #join all of the results into a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just copy-and-paste in an example job description that I have found for Junior Data Scientist at Verizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_description = \"Be a part of the team that identifies trends, emerging technologies and growth markets—all things that keep us at the forefront of innovation and drive our success Verizon Communications Inc. is a global leader in delivering broadband and other wireless and wireline communications services to mass market, business, government and wholesale customers. A Dow 30 company, Verizon employs a diverse workforce of more than 177,000 and last year generated consolidated revenues of $127 billion.  The Data Analyst – Junior Data Scientist will be part of an objective assurance and consulting team that is independently managed within Verizon Communications designed to add value and improve operations. The Internal Audit team assists the Audit Committee of the Board of Directors and Verizon management in accomplishing their objectives by bringing a systematic and disciplined approach to evaluate and improve the effectiveness of the overall control environment, risk management, and governance processes. The Internal Audit staff gains extensive exposure to diverse aspects of Verizon's business. These audit assignments include increasing levels of responsibilities and presentations to senior management, making Internal Audit an excellent place to work for high potential employees. Have you read Malcolm Gladwell’s books? Or the string of other authors solving business challenges through Data mining? Are you interested in a path to becoming a data scientist? Do you have what it takes to develop great business acumen? Leverage your critical thinking and problem solving skills while leveraging technical tools to analyze large complex data sets? If so then these responsibilities could be yours. As Wikipedia states ”Data Scientist have the ability to find and interpret rich data sources, manage large amounts of data despite hardware, software and bandwidth constraints, merge data sources together, ensure consistency of data-sets, create visualizations to aid in understanding data and building rich tools that enable others to work effectively.” POSITION RESPONSIBILITIES: The Data Analyst – Junior Data Scientist supports a high performance forensic and audit analytics team in its efforts to identify and drive data mining efforts through a risk based approach. The enthusiasm in this position helps mitigate fraud, identify misconduct and control gaps utilizing data mining and analysis.  Responsibilities include the following: Passion for growth and learning new techniques with vigor and enthusiasm. Strong individual contributor with top notch team collaboration skills.  Strong ability to independently and proactively initiate projects, hypothesize business transaction flows and fraudulent scenarios. Design, extract, normalize, analyze, review and automate analysis for Internal Audit utilizing enterprise data warehouse, extracts and data mining tools; coordinate with business for outside data source requirements. Design, develop, maintain and communicate visual dashboards. Design and develop ad-hoc analysis based on business requirement needs. Identify and use appropriate investigative and analytical technologies to interpret and verify results. Coordinate with business to ensure follow-up and resolution of exceptions including specific individual resolution as well as root-cause analysis and control gap identification. Review large software implementations to identify transaction flow gaps, design flaws and data integrity issues. Actively participates in the completion of department initiatives to support the development of a best-in-class Internal Audit function Maintain databases and related programs in a thorough and efficient manner. Qualifications Take part in training courses to increase skill set and technical capabilities in order to better serve the needs of the analytics team. Strong business analytical skills a must; ability to apply business logic to design and implement data mining techniques on large data sets. Projects with evidence of Creative and Critical thinking a must. Understanding of Data Warehousing is a must. Proficient in the use of Teradata SQL, MS SQL server (SSIS/SSAS experience preferred), Data Visualization (e.g., Tableau or other), MS Access, MS Excel, Visual Basic, and Sharepoint. Experience designing, developing, implementing and maintaining a database and programs to manage data analysis efforts. For internal candidates, experience with Verizon Wireless Enterprise Data Warehouse preferred. Working knowledge of ‘Big Data’ concepts and Hadoop/Hive, Teradata Aster, and R tools preferred. Working knowledge of building self-serve analytics tools for business users a plus. Working knowledge of statistical analysis, data mining and predictive modeling tools and techniques a plus. Working knowledge of application development and/or web development a plus. Demonstrated ability to work independently and within a team in a fast changing environment with changing priorities and changing time constraints. Strong interpersonal skills and ability to multi-task. Ability to interpret business requests as well as communicate findings in a user-friendly manner. Experience in normalizing data to ensure it is homogeneous and consistently formatted to enable sorting, query and analysis. Ability to write clear, concise reports and presentations with an ability to orally communicate effectively; organizational and documentation skills a must. An understanding of risk management methodology and factors. Consolidates issues for management level review; develops clear written recommendations, which require minimal editing; presents recommendations and resolves issues with management. BS/BA degree in Management Information Systems, Computer Science, Accounting, Business, Finance, Economics, Statistics or related field.  Masters degree a plus.  At least a 3.0/4.0 overall GPA or equivalent Requires a minimum of 4 years relevant work experience; Analytics, technology, auditing, accounting, finance, or economics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's the string. In the next cell, let's tokenize this string by word and sentence, which returns lists of all of the words and lists of all of the sentences respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jd_w_token = nltk.word_tokenize(job_description) #tokenize by word\n",
    "jd_sent_token = nltk.sent_tokenize(job_description) #tokenize by sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will attempt to tag each word with its part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_w = nltk.pos_tag(jd_w_token) #list of tuples of tagged words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create a frequency distribution, we'll then make a pandas dataframe out of the part-of-speech and distribution data and then clean and label it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = list(nltk.FreqDist(jd_w_token).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_df = pd.DataFrame(tag_w) #dataframe of words and parts of speech\n",
    "f_df = pd.DataFrame(freq) #dataframe of frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = pd.merge(w_df, f_df, on = 0, how = 'inner') #let's merge these into a single dataframe\n",
    "d.drop_duplicates([0], keep = 'last', inplace = True)\n",
    "d.rename(columns = {0: 'word','1_x':'part', '1_y':'count'}, inplace = True)\n",
    "d.sort_values(['part','count'], ascending = False, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap up these previous cells into a single function that takes an input string and outputs the 'word, part of speech, count' dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WordFrequency(string, return_tag = True):\n",
    "    w_token = nltk.word_tokenize(string) #tokenize by word\n",
    "    s_token = nltk.sent_tokenize(string) #tokenize by sentence\n",
    "    \n",
    "    tag_w = nltk.pos_tag(w_token) #list of tuples of tagged words\n",
    "    freq = list(nltk.FreqDist(w_token).items())\n",
    "    \n",
    "    w_df = pd.DataFrame(tag_w) #dataframe of words and parts of speech\n",
    "    f_df = pd.DataFrame(freq) #dataframe of frequency distribution\n",
    "    \n",
    "    d = pd.merge(w_df, f_df, on = 0, how = 'inner') #let's merge these into a single dataframe\n",
    "    d.drop_duplicates([0], keep = 'last', inplace = True)\n",
    "    d.rename(columns = {0: 'word','1_x':'part', '1_y':'count'}, inplace = True)\n",
    "    d.sort_values(['part','count'], ascending = False, axis = 0, inplace = True)\n",
    "    \n",
    "    if return_tag == True:\n",
    "        return tag_w\n",
    "    else:\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have d, a DataFrame of words, sorted by parts of speech, and then their count, let's select the verbs from this job description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = WordFrequency(single_string, return_tag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>part</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>using</td>\n",
       "      <td>VBG</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>spanning</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>performing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Translating</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>probing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>defining</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ensuring</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>pulling</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Creating</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Coding</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>conducting</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Outstanding</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>including</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>computing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>choosing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>making</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>creating</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>emphasizing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>embracing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>achieving</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word part  count\n",
       "101        using  VBG      3\n",
       "119     spanning  VBG      1\n",
       "154   performing  VBG      1\n",
       "168  Translating  VBG      1\n",
       "202      probing  VBG      1\n",
       "277     defining  VBG      1\n",
       "284     ensuring  VBG      1\n",
       "296      pulling  VBG      1\n",
       "320     Creating  VBG      1\n",
       "346       Coding  VBG      1\n",
       "347   conducting  VBG      1\n",
       "387  Outstanding  VBG      1\n",
       "410    including  VBG      1\n",
       "412    computing  VBG      1\n",
       "422     choosing  VBG      1\n",
       "452       making  VBG      1\n",
       "455     creating  VBG      1\n",
       "457  emphasizing  VBG      1\n",
       "461    embracing  VBG      1\n",
       "472    achieving  VBG      1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[(d['part'] == 'VBG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "result = cp.parse(WordFrequency(single_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.86"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.flesch_reading_ease(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textstat.flesch_kincaid_grade(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pattern.en as en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199         business\n",
       "96          analysis\n",
       "344         learning\n",
       "420              job\n",
       "298      environment\n",
       "341          machine\n",
       "424         category\n",
       "426         location\n",
       "459      development\n",
       "475          success\n",
       "1           position\n",
       "90           variety\n",
       "170        challenge\n",
       "179        framework\n",
       "191            Setup\n",
       "206           intent\n",
       "220             user\n",
       "262      acquisition\n",
       "263         approach\n",
       "265             skim\n",
       "266         creation\n",
       "268         database\n",
       "269     manipulation\n",
       "274          project\n",
       "275       management\n",
       "278            scope\n",
       "279           number\n",
       "280             type\n",
       "319          insight\n",
       "331     optimization\n",
       "332       simulation\n",
       "333       regression\n",
       "334         decision\n",
       "338          cluster\n",
       "357              eco\n",
       "358           system\n",
       "364    understanding\n",
       "371      Proficiency\n",
       "379        curiosity\n",
       "381          science\n",
       "384          ability\n",
       "385          climate\n",
       "386        ambiguity\n",
       "388    communication\n",
       "389     presentation\n",
       "394           degree\n",
       "407            focus\n",
       "428             term\n",
       "433            “add”\n",
       "438            order\n",
       "454            place\n",
       "460         training\n",
       "462        diversity\n",
       "466         guidance\n",
       "471             path\n",
       "488          pathway\n",
       "490           career\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[(d['part'] == 'NN')]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'is_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-6b887ce5490a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'is_number'"
     ]
    }
   ],
   "source": [
    "en.is_number(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
